{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import nltk.sentiment\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import j_acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = j_acquire.scrape_github_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'repo': 'openai/jukebox',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '**Status:** Archive (code is provided as-is, no updates expected)\\n\\n# Jukebox\\nCode for \"Jukebox: A Generative Model for Music\"\\n\\n[Paper](https://arxiv.org/abs/2005.00341) \\n[Blog](https://openai.com/blog/jukebox) \\n[Explorer](http://jukebox.openai.com/) \\n[Colab](https://colab.research.google.com/github/openai/jukebox/blob/master/jukebox/Interacting_with_Jukebox.ipynb) \\n\\n# Install\\nInstall the conda package manager from https://docs.conda.io/en/latest/miniconda.html    \\n    \\n``` \\n# Required: Sampling\\nconda create --name jukebox python=3.7.5\\nconda activate jukebox\\nconda install mpi4py=3.0.3 # if this fails, try: pip install mpi4py==3.0.3\\nconda install pytorch=1.4 torchvision=0.5 cudatoolkit=10.0 -c pytorch\\ngit clone https://github.com/openai/jukebox.git\\ncd jukebox\\npip install -r requirements.txt\\npip install -e .\\n\\n# Required: Training\\nconda install av=7.0.01 -c conda-forge \\npip install ./tensorboardX\\n \\n# Optional: Apex for faster training with fused_adam\\nconda install pytorch=1.1 torchvision=0.3 cudatoolkit=10.0 -c pytorch\\npip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\\n```\\n\\n# Sampling\\n## Sampling from scratch\\nTo sample normally, run the following command. Model can be `5b`, `5b_lyrics`, `1b_lyrics`\\n``` \\npython jukebox/sample.py --model=5b_lyrics --name=sample_5b --levels=3 --sample_length_in_seconds=20 \\\\\\n--total_sample_length_in_seconds=180 --sr=44100 --n_samples=6 --hop_fraction=0.5,0.5,0.125\\n```\\n``` \\npython jukebox/sample.py --model=1b_lyrics --name=sample_1b --levels=3 --sample_length_in_seconds=20 \\\\\\n--total_sample_length_in_seconds=180 --sr=44100 --n_samples=16 --hop_fraction=0.5,0.5,0.125\\n```\\nThe above generates the first `sample_length_in_seconds` seconds of audio from a song of total length `total_sample_length_in_seconds`.\\n\\nThe samples decoded from each level are stored in `{name}/level_{level}`. \\nYou can also view the samples as an html with the aligned lyrics under `{name}/level_{level}/index.html`. \\nRun `python -m http.server` and open the html through the server to see the lyrics animate as the song plays.  \\nA summary of all sampling data including zs, x, labels and sampling_kwargs is stored in `{name}/level_{level}/data.pth.tar`.\\n\\nThe hps are for a V100 GPU with 16 GB GPU memory. The `1b_lyrics`, `5b`, and `5b_lyrics` top-level priors take up \\n3.8 GB, 10.3 GB, and 11.5 GB, respectively. The peak memory usage to store transformer key, value cache is about 400 MB \\nfor `1b_lyrics` and 1 GB for `5b_lyrics` per sample. If you are having trouble with CUDA OOM issues, try `1b_lyrics` or \\ndecrease `max_batch_size` in sample.py, and `--n_samples` in the script call.\\n\\nOn a V100, it takes about 3 hrs to fully sample 20 seconds of music. Since this is a long time, it is recommended to use `n_samples > 1` so you can generate as many samples as possible in parallel. The 1B lyrics and upsamplers can process 16 samples at a time, while 5B can fit only up to 3. Since the vast majority of time is spent on upsampling, we recommend using a multiple of 3 less than 16 like `--n_samples 15` for `5b_lyrics`. This will make the top-level generate samples in groups of three while upsampling is done in one pass.\\n\\nTo continue sampling from already generated codes for a longer duration, you can run\\n```\\npython jukebox/sample.py --model=5b_lyrics --name=sample_5b --levels=3 --mode=continue \\\\\\n--codes_file=sample_5b/level_0/data.pth.tar --sample_length_in_seconds=40 --total_sample_length_in_seconds=180 \\\\\\n--sr=44100 --n_samples=6 --hop_fraction=0.5,0.5,0.125\\n```\\nHere, we take the 20 seconds samples saved from the first sampling run at `sample_5b/level_0/data.pth.tar` and continue by adding 20 more seconds. \\n\\nYou could also continue directly from the level 2 saved outputs, just pass `--codes_file=sample_5b/level_2/data.pth.tar`.\\n Note this will upsample the full 40 seconds song at the end.\\n\\nIf you stopped sampling at only the first level and want to upsample the saved codes, you can run\\n```\\npython jukebox/sample.py --model=5b_lyrics --name=sample_5b --levels=3 --mode=upsample \\\\\\n--codes_file=sample_5b/level_2/data.pth.tar --sample_length_in_seconds=20 --total_sample_length_in_seconds=180 \\\\\\n--sr=44100 --n_samples=6 --hop_fraction=0.5,0.5,0.125\\n```\\nHere, we take the 20 seconds samples saved from the first sampling run at `sample_5b/level_2/data.pth.tar` and upsample the lower two levels.\\n\\n## Prompt with your own music\\nIf you want to prompt the model with your own creative piece or any other music, first save them as wave files and run\\n```\\npython jukebox/sample.py --model=5b_lyrics --name=sample_5b_prompted --levels=3 --mode=primed \\\\\\n--audio_file=path/to/recording.wav,awesome-mix.wav,fav-song.wav,etc.wav --prompt_length_in_seconds=12 \\\\\\n--sample_length_in_seconds=20 --total_sample_length_in_seconds=180 --sr=44100 --n_samples=6 --hop_fraction=0.5,0.5,0.125\\n```\\nThis will load the four files, tile them to fill up to `n_samples` batch size, and prime the model with the first `prompt_length_in_seconds` seconds.\\n\\n# Training\\n## VQVAE\\nTo train a small vqvae, run\\n```\\nmpiexec -n {ngpus} python jukebox/train.py --hps=small_vqvae --name=small_vqvae --sample_length=262144 --bs=4 \\\\\\n--audio_files_dir={audio_files_dir} --labels=False --train --aug_shift --aug_blend\\n```\\nHere, `{audio_files_dir}` is the directory in which you can put the audio files for your dataset, and `{ngpus}` is number of GPU\\'s you want to use to train. \\nThe above trains a two-level VQ-VAE with `downs_t = (5,3)`, and `strides_t = (2, 2)` meaning we downsample the audio by `2**5 = 32` to get the first level of codes, and `2**8 = 256` to get the second level codes.  \\nCheckpoints are stored in the `logs` folder. You can monitor the training by running Tensorboard\\n```\\ntensorboard --logdir logs\\n```\\n    \\n## Prior\\n### Train prior or upsamplers\\nOnce the VQ-VAE is trained, we can restore it from its saved checkpoint and train priors on the learnt codes. \\nTo train the top-level prior, we can run\\n\\n```\\nmpiexec -n {ngpus} python jukebox/train.py --hps=small_vqvae,small_prior,all_fp16,cpu_ema --name=small_prior \\\\\\n--sample_length=2097152 --bs=4 --audio_files_dir={audio_files_dir} --labels=False --train --test --aug_shift --aug_blend \\\\\\n--restore_vqvae=logs/small_vqvae/checkpoint_latest.pth.tar --prior --levels=2 --level=1 --weight_decay=0.01 --save_iters=1000\\n```\\n\\nTo train the upsampler, we can run\\n```\\nmpiexec -n {ngpus} python jukebox/train.py --hps=small_vqvae,small_upsampler,all_fp16,cpu_ema --name=small_upsampler \\\\\\n--sample_length=262144 --bs=4 --audio_files_dir={audio_files_dir} --labels=False --train --test --aug_shift --aug_blend \\\\\\n--restore_vqvae=logs/small_vqvae/checkpoint_latest.pth.tar --prior --levels=2 --level=0 --weight_decay=0.01 --save_iters=1000\\n```\\nWe pass `sample_length = n_ctx * downsample_of_level` so that after downsampling the tokens match the n_ctx of the prior hps. \\nHere, `n_ctx = 8192` and `downsamples = (32, 256)`, giving `sample_lengths = (8192 * 32, 8192 * 256) = (65536, 2097152)` respectively for the bottom and top level. \\n\\n### Learning rate annealing\\nTo get the best sample quality anneal the learning rate to 0 near the end of training. To do so, continue training from the latest \\ncheckpoint and run with\\n```\\n--restore_prior=\"path/to/checkpoint\" --lr_use_linear_decay --lr_start_linear_decay={already_trained_steps} --lr_decay={decay_steps_as_needed}\\n```\\n\\n### Reuse pre-trained VQ-VAE and train top-level prior on new dataset from scratch.\\n#### Train without labels\\nOur pre-trained VQ-VAE can produce compressed codes for a wide variety of genres of music, and the pre-trained upsamplers \\ncan upsample them back to audio that sound very similar to the original audio.\\nTo re-use these for a new dataset of your choice, you can retrain just the top-level  \\n\\nTo train top-level on a new dataset, run\\n```\\nmpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,small_prior,all_fp16,cpu_ema --name=pretrained_vqvae_small_prior \\\\\\n--sample_length=1048576 --bs=4 --aug_shift --aug_blend --audio_files_dir={audio_files_dir} \\\\\\n--labels=False --train --test --prior --levels=3 --level=2 --weight_decay=0.01 --save_iters=1000\\n```\\nTraining the `small_prior` with a batch size of 2, 4, and 8 requires 6.7 GB, 9.3 GB, and 15.8 GB of GPU memory, respectively. A few days to a week of training typically yields reasonable samples when the dataset is homogeneous (e.g. all piano pieces, songs of the same style, etc).\\n\\nNear the end of training, follow [this](#learning-rate-annealing) to anneal the learning rate to 0\\n\\n#### Sample from new model\\nYou can then run sample.py with the top-level of our models replaced by your new model. To do so,\\n- Add an entry `my_model=(\"vqvae\", \"upsampler_level_0\", \"upsampler_level_1\", \"small_prior\")` in `MODELS` in `make_models.py`. \\n- Update the `small_prior` dictionary in `hparams.py` to include `restore_prior=\\'path/to/checkpoint\\'`. If you\\nyou changed any hps directly in the command line script (eg:`heads`), make sure to update them in the dictionary too so \\nthat `make_models` restores our checkpoint correctly.\\n- Run sample.py as outlined in the sampling section, but now with `--model=my_model` \\n\\n#### Train with labels \\nTo train with you own metadata for your audio files, implement `get_metadata` in `data/files_dataset.py` to return the \\n`artist`, `genre` and `lyrics` for a given audio file. For now, you can pass `\\'\\'` for lyrics to not use any lyrics.\\n\\nFor training with labels, we\\'ll use `small_labelled_prior` in `hparams.py`, and we set `labels=True,labels_v3=True`. \\nWe use 2 kinds of labels information:\\n- Artist/Genre: \\n  - For each file, we return an artist_id and a list of genre_ids. The reason we have a list and not a single genre_id \\n  is that in v2, we split genres like `blues_rock` into a bag of words `[blues, rock]`, and we pass atmost \\n  `max_bow_genre_size` of those, in `v3` we consider it as a single word and just set `max_bow_genre_size=1`.\\n  - Update the `v3_artist_ids` and `v3_genre_ids` to use ids from your new dataset. \\n  - In `small_labelled_prior`, set the hps `y_bins = (number_of_genres, number_of_artists)` and `max_bow_genre_size=1`. \\n- Timing: \\n  - For each chunk of audio, we return the `total_length` of the song, the `offset` the current audio chunk is at and \\n  the `sample_length` of the audio chunk. We have three timing embeddings: total_length, our current position, and our \\n  current position as a fraction of the total length, and we divide the range of these values into `t_bins` discrete bins. \\n  - In `small_labelled_prior`, set the hps `min_duration` and `max_duration` to be the shortest/longest duration of audio \\n  files you want for your dataset, and `t_bins` for how many bins you want to discretize timing information into. Note \\n  `min_duration * sr` needs to be at least `sample_length` to have an audio chunk in it.\\n\\nAfter these modifications, to train a top-level with labels, run\\n```\\nmpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,small_labelled_prior,all_fp16,cpu_ema --name=pretrained_vqvae_small_prior_labels \\\\\\n--sample_length=1048576 --bs=4 --aug_shift --aug_blend --audio_files_dir={audio_files_dir} \\\\\\n--labels=True --train --test --prior --levels=3 --level=2 --weight_decay=0.01 --save_iters=1000\\n```\\n\\nFor sampling, follow same instructions as [above](#sample-from-new-model) but use `small_labelled_prior` instead of `small_prior`.  \\n\\n#### Train with lyrics\\nTo train in addition with lyrics, update `get_metadata` in `data/files_dataset.py` to return `lyrics` too.\\nFor training with lyrics, we\\'ll use `small_single_enc_dec_prior` in `hparams.py`. \\n- Lyrics: \\n  - For each file, we linearly align the lyric characters to the audio, find the position in lyric that corresponds to \\n  the midpoint of our audio chunk, and pass a window of `n_tokens` lyric characters centred around that. \\n  - In `small_single_enc_dec_prior`, set the hps `use_tokens=True` and `n_tokens` to be the number of lyric characters \\n  to use for an audio chunk. Set it according to the `sample_length` you\\'re training on so that its large enough that \\n  the lyrics for an audio chunk are almost always found inside a window of that size.\\n  - If you use a non-English vocabulary, update `text_processor.py` with your new vocab and set\\n  `n_vocab = number of characters in vocabulary` accordingly in `small_single_enc_dec_prior`. In v2, we had a `n_vocab=80` \\n  and in v3 we missed `+` and so `n_vocab=79` of characters. \\n\\nAfter these modifications, to train a top-level with labels and lyrics, run\\n```\\nmpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,small_single_enc_dec_prior,all_fp16,cpu_ema --name=pretrained_vqvae_small_single_enc_dec_prior_labels \\\\\\n--sample_length=786432 --bs=4 --aug_shift --aug_blend --audio_files_dir={audio_files_dir} \\\\\\n--labels=True --train --test --prior --levels=3 --level=2 --weight_decay=0.01 --save_iters=1000\\n```\\nTo simplify hps choices, here we used a `single_enc_dec` model like the `1b_lyrics` model that combines both encoder and \\ndecoder of the transformer into a single model. We do so by merging the lyric vocab and vq-vae vocab into a single \\nlarger vocab, and flattening the lyric tokens and the vq-vae codes into a single sequence of length `n_ctx + n_tokens`. \\nThis uses `attn_order=12` which includes `prime_attention` layers with keys/values from lyrics and queries from audio. \\nIf you instead want to use a model with the usual encoder-decoder style transformer, use `small_sep_enc_dec_prior`.\\n\\nFor sampling, follow same instructions as [above](#sample-from-new-model) but use `small_single_enc_dec_prior` instead of \\n`small_prior`. To also get the alignment between lyrics and samples in the saved html, you\\'ll need to set `alignment_layer` \\nand `alignment_head` in `small_single_enc_dec_prior`. To find which layer/head is best to use, run a forward pass on a training example,\\nsave the attention weight tensors for all prime_attention layers, and pick the (layer, head) which has the best linear alignment \\npattern between the lyrics keys and music queries. \\n\\n### Fine-tune pre-trained top-level prior to new style(s)\\nPreviously, we showed how to train a small top-level prior from scratch. Assuming you have a GPU with at least 15 GB of memory and support for fp16, you could fine-tune from our pre-trained 1B top-level prior. Here are the steps:\\n\\n- Support `--labels=True` by implementing `get_metadata` in `jukebox/data/files_dataset.py` for your dataset.\\n- Add new entries in `jukebox/data/ids`. We recommend replacing existing mappings (e.g. rename `\"unknown\"`, etc with styles of your choice). This uses the pre-trained style vectors as initialization and could potentially save some compute.\\n\\nAfter these modifications, run \\n```\\nmpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,prior_1b_lyrics,all_fp16,cpu_ema --name=finetuned \\\\\\n--sample_length=1048576 --bs=1 --aug_shift --aug_blend --audio_files_dir={audio_files_dir} \\\\\\n--labels=True --train --test --prior --levels=3 --level=2 --weight_decay=0.01 --save_iters=1000\\n```\\nTo get the best sample quality, it is recommended to anneal the learning rate in the end. Training the 5B top-level requires GPipe which is not supported in this release.\\n\\n# Citation\\n\\nPlease cite using the following bibtex entry:\\n\\n```\\n@article{dhariwal2020jukebox,\\n  title={Jukebox: A Generative Model for Music},\\n  author={Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},\\n  journal={arXiv preprint arXiv:2005.00341},\\n  year={2020}\\n}\\n```\\n\\n# License \\n[Noncommercial Use License](./LICENSE) \\n\\nIt covers both released code and weights. \\n\\n'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def basic_clean(df, col):\n",
    "#     '''\n",
    "#     This function takes in a df and a string for a column and\n",
    "#     returns the df with a new column named 'basic_clean' with the\n",
    "#     passed column text normalized.\n",
    "#     '''\n",
    "#     df['basic_clean'] = df[col].str.lower()\\\n",
    "#                     .replace(r'[^\\w\\s]', '', regex=True)\\\n",
    "#                     .str.normalize('NFKC')\\\n",
    "#                     .str.encode('ascii', 'ignore')\\\n",
    "#                     .str.decode('utf-8', 'ignore')\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(df, col):\n",
    "#     '''\n",
    "#     This function takes in a df and a string for a column and\n",
    "#     returns a df with a new column named 'clean_tokes' with the\n",
    "#     passed column text tokenized and in a list.\n",
    "#     '''\n",
    "#     tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "#     df['clean_tokes'] = df[col].apply(tokenizer.tokenize)\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def stem(df, col):\n",
    "#     '''\n",
    "#     This function takes in a df and a string for a column name and\n",
    "#     returns a df with a new column named 'stemmed'.\n",
    "#     '''\n",
    "#     # Create porter stemmer\n",
    "#     ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "#     # Stem each token from our clean_tokes Series of lists\n",
    "#     stems = df[col].apply(lambda row: [ps.stem(word) for word in row])\n",
    "    \n",
    "#     # Join our cleaned, stemmed lists of words back into sentences\n",
    "#     df['stemmed'] = stems.str.join(' ')\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def lemmatize(df, col):\n",
    "#     '''\n",
    "#     This function takes in a df and a string for column name and\n",
    "#     returns the original df with a new column called 'lemmatized'.\n",
    "#     '''\n",
    "#     # Create the lemmatizer\n",
    "#     wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "#     # Lemmatize each token from our clean_tokes Series of lists\n",
    "#     lemmas = df[col].apply(lambda row: [wnl.lemmatize(word) for word in row])\n",
    "    \n",
    "#     # Join the cleaned and lemmatized tokens back into sentences\n",
    "#     df['lemmatized'] = lemmas.str.join(' ')\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def remove_stopwords(df, col):\n",
    "#     '''\n",
    "#     This function takes in a df and a string for column name and \n",
    "#     returns the df with a new column named 'clean' with stopwords removed.\n",
    "#     '''\n",
    "#     # Create stopword_list\n",
    "#     stopword_list = stopwords.words('english')\n",
    "    \n",
    "#     # Split words in column\n",
    "#     words = df[col].str.split()\n",
    "    \n",
    "#     # Check each word in each row of the column against stopword_list and return only those that are not in list\n",
    "#     filtered_words = words.apply(lambda row: [word for word in row if word not in stopword_list])\n",
    "    \n",
    "#     # Create new column of words that have stopwords removed\n",
    "#     df['clean_' + col] = filtered_words.str.join(' ')\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prep_data(df):\n",
    "#     '''\n",
    "#     This function takes in the news articles df and\n",
    "#     returns the df with original columns plus cleaned\n",
    "#     and lemmatized content without stopwords.\n",
    "#     '''\n",
    "#     # Do basic clean on article content\n",
    "#     df = basic_clean(df, 'readme_contents')\n",
    "    \n",
    "#     # Tokenize clean article content\n",
    "#     df = tokenize(df, 'basic_clean')\n",
    "    \n",
    "#     # Stem cleaned and tokenized article content\n",
    "#     df = stem(df, 'clean_tokes')\n",
    "    \n",
    "#     # Remove stopwords from Lemmatized article content\n",
    "#     df = remove_stopwords(df, 'stemmed')\n",
    "    \n",
    "#     # Lemmatize cleaned and tokenized article content\n",
    "#     df = lemmatize(df, 'clean_tokes')\n",
    "    \n",
    "#     # Remove stopwords from Lemmatized article content\n",
    "#     df = remove_stopwords(df, 'lemmatized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>openai/jukebox</td>\n",
       "      <td>Python</td>\n",
       "      <td>**Status:** Archive (code is provided as-is, n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             repo language                                    readme_contents\n",
       "0  openai/jukebox   Python  **Status:** Archive (code is provided as-is, n..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    **status:** archive (code is provided as-is, n...\n",
       "Name: readme_contents, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['readme_contents'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e1b138f4251b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\[.*?\\]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadme_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/re.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "re.findall(r'\\[.*?\\]', df.readme_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "findall() missing 1 required positional argument: 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-880924096393>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadme_contents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\[.*?\\]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: findall() missing 1 required positional argument: 'string'"
     ]
    }
   ],
   "source": [
    "for line in df.readme_contents:\n",
    "    re.findall(r'\\[.*?\\]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
