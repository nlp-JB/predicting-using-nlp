{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# code to regulate the size of plots for the rest of notebook\n",
    "plt.rc('figure', figsize=(13, 10))\n",
    "plt.rc('font', size=13)\n",
    "\n",
    "import j_acquire\n",
    "import j_prep\n",
    "import explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project\n",
    "\n",
    "### The Goal: Build a model that can predict what programming language a repository is, given the text of the README file.\n",
    "\n",
    "## Planning\n",
    "\n",
    "Possible Hypotheses:\n",
    "1. The number of links in a readme file is proportional to the popularity of a language\n",
    "2. There will be files eanding in extensions particular to a language that will indicate which language the project was prepared in (ie, .py files will be in python projects)\n",
    "3. \n",
    "\n",
    "## Acquisition\n",
    "\n",
    "We chose to look at 11 topics and scrape the readme files of the 30 most starred repos in each topic.\n",
    "- Topics:\n",
    "    1. algorithm\n",
    "    2. bots\n",
    "    3. data-visualization\n",
    "    4. deep_learning\n",
    "    5. javascript\n",
    "    6. jupyter_notebook\n",
    "    7. machine_learning\n",
    "    8. nlp\n",
    "    9. python\n",
    "    10. repo_source\n",
    "    11. testing\n",
    "\n",
    "We saved the source code of each topic page into a text file for future access and to avoid any access issues. Our create_large_df function scrapes the repo names from each text file, applies Zach's scrape_github_data function to acquire the language type and readme in each repo, drops any null rows, and then combines everything into one large dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 298 rows in the resulting dataframe.\n"
     ]
    }
   ],
   "source": [
    "text_files = ['algorithm', 'bots', 'data-visualization',\n",
    "              'deep_learning', 'javascript', 'jupyter_notebook',\n",
    "              'machine_learning', 'nlp', 'python',\n",
    "              'repo_source', 'testing']\n",
    "\n",
    "df = j_acquire.create_large_df(text_files)\n",
    "print(f'There are {df.shape[0]} rows in the resulting dataframe.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Our prep_readme_data function accomplishes all of the basic clean operations, adds new columns tokenizing, stemming, and lemmatizing the readme contents. \n",
    "\n",
    "We also added a number of new features to make modeling easier. \n",
    "- General languages: grouping all the popular languages together, and adding the singletons into an 'other' category\n",
    "- Number of individual words\n",
    "- Number of unique words\n",
    "- Number of links\n",
    "- Counts of each .py, .js & .ipynb extensions\n",
    "\n",
    "This file was saved a .csv for easy access and to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('prepared_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we created a dataframe of every word in each of the popular languages: Python, JavaScript, Jupyter Notebooks, C++, TypeScript, Java, other, and all languages together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = explore.make_word_counts_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we created a vectorized dataframe with each of the words in the readmes are individual columns, and their importance in each readme are the values. We also added the new columns we created to this datafame for modeling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_df = explore.make_vectorized_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the 10 most common words in our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "de         1529\n",
       "python     1371\n",
       "data       1247\n",
       "use        1237\n",
       "model      1171\n",
       "using      1035\n",
       "1           990\n",
       "code        930\n",
       "install     883\n",
       "network     762\n",
       "Name: all, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.sort_values(by='all', ascending=False)['all'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the 10 most common words in each language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "python     922\n",
       "model      748\n",
       "data       437\n",
       "use        407\n",
       "install    365\n",
       "code       351\n",
       "0          348\n",
       "using      340\n",
       "_          331\n",
       "library    322\n",
       "Name: python, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python\n",
    "word_counts.sort_values(by='python', ascending=False)['python'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "de            1496\n",
       "e              488\n",
       "que            379\n",
       "const          365\n",
       "covid19        361\n",
       "1              344\n",
       "para           336\n",
       "use            329\n",
       "javascript     318\n",
       "um             308\n",
       "Name: javascript, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JavaScript\n",
    "word_counts.sort_values(by='javascript', ascending=False)['javascript'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data          280\n",
       "python        279\n",
       "tensorflow    253\n",
       "using         240\n",
       "model         214\n",
       "use           209\n",
       "network       205\n",
       "notebook      199\n",
       "learning      193\n",
       "build         190\n",
       "Name: jupyter, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jupyter Notebook\n",
    "word_counts.sort_values(by='jupyter', ascending=False)['jupyter'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "build      179\n",
       "1          134\n",
       "pytorch    133\n",
       "onnx       130\n",
       "c          113\n",
       "support    112\n",
       "cntk       109\n",
       "using      107\n",
       "given      106\n",
       "python     102\n",
       "Name: c_plus, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C++\n",
    "word_counts.sort_values(by='c_plus', ascending=False)['c_plus'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "component    67\n",
       "using        61\n",
       "test         57\n",
       "run          55\n",
       "storybook    52\n",
       "build        47\n",
       "use          46\n",
       "yarn         46\n",
       "npm          46\n",
       "data         44\n",
       "Name: typescript, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TypeScript\n",
    "word_counts.sort_values(by='typescript', ascending=False)['typescript'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tree      106\n",
       "binary     93\n",
       "dp         91\n",
       "two        88\n",
       "search     84\n",
       "dfs        69\n",
       "table      67\n",
       "hash       56\n",
       "list       55\n",
       "string     46\n",
       "Name: java, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Java\n",
    "word_counts.sort_values(by='java', ascending=False)['java'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "al          374\n",
       "et          373\n",
       "network     218\n",
       "deep        182\n",
       "learning    176\n",
       "go          163\n",
       "code        160\n",
       "neural      151\n",
       "2016        147\n",
       "data        136\n",
       "Name: other, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Everything else\n",
    "word_counts.sort_values(by='other', ascending=False)['other'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During our exploration, we learned that not all words are relevant to the majority of the readmes in our dataset. Therefore, we decided to handle this as such: \n",
    "1. We aggregated similar words together (ie, we added together the values for build, building and built; etc.).\n",
    "2. We dropped all the words where the mean importance was less than 5%.\n",
    "3. We added a 'politeness' score, adding together each of the 'polite' words like please and thank you.\n",
    "\n",
    "Then, we split and scaled this dataset, and went ahead and made our prediction dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate similar words\n",
    "vectorized_df = explore.aggregate_columns(vectorized_df)\n",
    "# Split & scale data, and make predictions dataframes\n",
    "X_train_scaled, X_test_scaled, y_train, y_test, train_predictions, test_predictions = explore.get_splits(df, vectorized_df)\n",
    "# Drop all words that aren't important\n",
    "X_train_reduced, X_test_reduced = explore.prep_vectorized_df(X_train_scaled, X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((208, 202), (90, 202))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reduced.shape, X_test_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding back together X_train & y_train for exploration\n",
    "X_train_reduced['y'] = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing\n",
    "\n",
    "- $H_0$ there is no difference between number of links for javascript repos and the overall average number of links.\n",
    "- $H_a$ there is a difference between number of links for javascript repos and the overall average number of links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 1.512\n",
      "p = 0.138\n"
     ]
    }
   ],
   "source": [
    "javascript = X_train_reduced[X_train_reduced['y'] == 'JavaScript']\n",
    "t, p = stats.ttest_1samp(javascript.link_counts, X_train_reduced.link_counts.mean())\n",
    "\n",
    "print(f't = {t:.3f}')\n",
    "print(f'p = {p:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "if p < alpha:\n",
    "    print(f'P value of {p:.3f} is less than alpha, so we fail to reject the hypothesis.')\n",
    "el"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $H_0$ there is no difference between number of words for python repos and the overall average number of words for all repos.\n",
    "- $H_a$ there is a difference between number of words for python repos and the overall average number of words for all repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python = df[df.gen_language == 'Python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and make test predictions dfs\n",
    "X_train_scaled, X_test_scaled, y_train, y_test, train_predictions, test_predictions = explore.get_splits(df, vectorized_df)\n",
    "# reduce the vectorized dataframe \n",
    "X_train_reduced, X_test_reduced = explore.prep_vectorized_df(X_train_scaled, X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions, test_predictions = modeling.make_predictions_df(X_train_reduced, X_test_reduced, y_train, train_predictions, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling.train_evaluation(train_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
